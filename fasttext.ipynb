{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PRbGDJMOYfOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cdcf97b-0cf4-46fa-f56c-868ac8f6105e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import resample\n",
        "import re"
      ],
      "metadata": {
        "id": "8g0zaOUfYgoy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "kzGo3iTjYkII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e9ae53-83d9-4e86-b8b3-eb4741e65f38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim"
      ],
      "metadata": {
        "id": "M-1BCLsCY_wW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20254a5a-6f78-4a0a-f13c-b3adbb5a7555"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/SemEval2016-Task6-raw-annotations-stance.csv')\n",
        "print(df.head)"
      ],
      "metadata": {
        "id": "1SBoDgQ6Yn_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0eb4330-78a6-4527-cf4d-c1bd27691f35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of         Worker ID  Instance ID                    Target  \\\n",
            "0        worker_1       2370.0  Legalization of Abortion   \n",
            "1        worker_2       2370.0  Legalization of Abortion   \n",
            "2        worker_3       2370.0  Legalization of Abortion   \n",
            "3        worker_4       2370.0  Legalization of Abortion   \n",
            "4        worker_5       2370.0  Legalization of Abortion   \n",
            "...           ...          ...                       ...   \n",
            "53094  worker_113      10951.0           Hillary Clinton   \n",
            "53095   worker_85      10951.0           Hillary Clinton   \n",
            "53096   worker_56      10951.0           Hillary Clinton   \n",
            "53097   worker_84      10951.0           Hillary Clinton   \n",
            "53098   worker_79      10951.0           Hillary Clinton   \n",
            "\n",
            "                                                   Tweet   Stance  \\\n",
            "0      Thank you for another day of life Lord. #Chris...  AGAINST   \n",
            "1      Thank you for another day of life Lord. #Chris...    FAVOR   \n",
            "2      Thank you for another day of life Lord. #Chris...  AGAINST   \n",
            "3      Thank you for another day of life Lord. #Chris...  AGAINST   \n",
            "4      Thank you for another day of life Lord. #Chris...  AGAINST   \n",
            "...                                                  ...      ...   \n",
            "53094  @TheAtlantic cause  #Hillary Clinton is a LYIN...  AGAINST   \n",
            "53095  @TheAtlantic cause  #Hillary Clinton is a LYIN...  AGAINST   \n",
            "53096  @TheAtlantic cause  #Hillary Clinton is a LYIN...  AGAINST   \n",
            "53097  @TheAtlantic cause  #Hillary Clinton is a LYIN...  AGAINST   \n",
            "53098  @TheAtlantic cause  #Hillary Clinton is a LYIN...  AGAINST   \n",
            "\n",
            "      Opinion towards  \n",
            "0              TARGET  \n",
            "1              TARGET  \n",
            "2               OTHER  \n",
            "3               OTHER  \n",
            "4              TARGET  \n",
            "...               ...  \n",
            "53094          TARGET  \n",
            "53095           OTHER  \n",
            "53096          TARGET  \n",
            "53097          TARGET  \n",
            "53098          TARGET  \n",
            "\n",
            "[53099 rows x 6 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['Stance'] = encoder.fit_transform(df['Stance'])"
      ],
      "metadata": {
        "id": "aEVth4kLYsVg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5hoBnIBdaD0",
        "outputId": "66c6a93f-1f1c-4171-a106-359a1cbbe05e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4391719 sha256=766c92702429d9edd5de4403b23bc725a96edfc4fd751d73d4f42b13f230ab06\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext"
      ],
      "metadata": {
        "id": "DwT456rDdTZI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=fasttext.load_model('/content/drive/MyDrive/Twitter-semeval2016-master/Twitter-semeval2016-master/random_partitions_of_unlabeled_corpus/cc.en.300.bin')\n"
      ],
      "metadata": {
        "id": "0st8KHECduR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b4f810-bd23-447d-9888-1c99164bcccc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_fasttext(df,model):\n",
        "\n",
        "\n",
        "    # Separate the majority and minority classes\n",
        "    df_majority = df[df['Stance'] == 0]\n",
        "    df_minority = df[df['Stance'] == 1]\n",
        "\n",
        "    # Resample the majority class to match the number of samples in the minority class\n",
        "    df_majority_resampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
        "\n",
        "    # Concatenate the resampled majority class with the minority class\n",
        "    df_resampled = pd.concat([df_majority_resampled, df_minority])\n",
        "\n",
        "    # Shuffle the rows of the resampled dataset\n",
        "    df_resampled = df_resampled.sample(frac=1, random_state=42)\n",
        "    df = df_resampled\n",
        "\n",
        "    def clean_data(review):\n",
        "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
        "        review = review.lower()\n",
        "        return review\n",
        "\n",
        "    df['Tweet'] = df['Tweet'].apply(clean_data)\n",
        "\n",
        "    def remove_stop_words(review):\n",
        "        review_minus_sw = []\n",
        "        stop_words = stopwords.words('english')\n",
        "        review = review.split()\n",
        "        review = [review_minus_sw.append(word) for word in review if word not in stop_words]\n",
        "        review = ' '.join(review_minus_sw)\n",
        "        return review\n",
        "\n",
        "    df['Tweet'] = df['Tweet'].apply(remove_stop_words)\n",
        "\n",
        "    def lematize(review):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        review = review.split()\n",
        "        review = [lemmatizer.lemmatize(word) for word in review]\n",
        "        review = ' '.join(review)\n",
        "        return review\n",
        "\n",
        "    df['Tweet'] = df['Tweet'].apply(lematize)\n",
        "\n",
        "    message = df['Tweet']\n",
        "    stance = df['Stance']\n",
        "    X_train_embeddings = np.array([model.get_word_vector(x) for x in message])\n",
        "    #input_data = pad_sequences(word_emd, maxlen=100, padding='post')\n",
        "    #train_embeds = [[0] * 300] * len(message)\n",
        "    #for idx, tweet in enumerate(message):\n",
        "    #  for word in tweet:\n",
        "    #    train_embeds[idx] += model[word]\n",
        "    #train_embeds=np.array(train_embeds)\n",
        "    X_train, X_test , y_train, y_test = train_test_split(X_train_embeddings, stance, test_size=0.20)\n",
        "\n",
        "\n",
        "    max_length = 300\n",
        "    vocab_size = 10000\n",
        "\n",
        "    def define_rnn_model():\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size, 300, input_length=max_length))\n",
        "        model.add(LSTM(64))\n",
        "        model.add(Dense(1, activation='relu'))\n",
        "        return model\n",
        "\n",
        "    # call the define_rnn_model function to create the model object\n",
        "    model = define_rnn_model()\n",
        "\n",
        "    # print the model summary\n",
        "    model.summary()\n",
        "\n",
        "    # calling adam optimizer\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # visualizing each eppoch\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "    # caluclating loss and accuracy\n",
        "    loss, accuracy = model.evaluate(X_test,y_test)\n",
        "\n",
        "    # printing accuracy\n",
        "    print('Testing Accuracy is {} '.format(accuracy*100))\n",
        "\n",
        "    y_predict = (model.predict(X_test)>=0.5).astype(int)\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(y_test,y_predict,target_names = [\"Against\",\"Favor\"]))\n"
      ],
      "metadata": {
        "id": "1DIU9QmzZF3Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*****************Feminist Movement******************\")\n",
        "rnn_fasttext(df[df['Target'] == 'Feminist Movement'],model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzzaZELgQBue",
        "outputId": "6217bc60-9f32-4d10-f54a-40f9bb193938"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************Feminist Movement******************\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 300)          3000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                93440     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,093,505\n",
            "Trainable params: 3,093,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "177/177 [==============================] - 98s 533ms/step - loss: 0.7115 - accuracy: 0.4770 - val_loss: 0.6967 - val_accuracy: 0.4929\n",
            "Epoch 2/10\n",
            "177/177 [==============================] - 94s 531ms/step - loss: 0.6952 - accuracy: 0.4940 - val_loss: 0.6931 - val_accuracy: 0.5071\n",
            "Epoch 3/10\n",
            "177/177 [==============================] - 91s 515ms/step - loss: 0.6958 - accuracy: 0.4922 - val_loss: 0.6936 - val_accuracy: 0.4929\n",
            "Epoch 4/10\n",
            "177/177 [==============================] - 95s 539ms/step - loss: 0.6977 - accuracy: 0.5011 - val_loss: 0.6980 - val_accuracy: 0.5071\n",
            "Epoch 5/10\n",
            "177/177 [==============================] - 94s 532ms/step - loss: 0.6963 - accuracy: 0.5039 - val_loss: 0.6948 - val_accuracy: 0.4929\n",
            "Epoch 6/10\n",
            "177/177 [==============================] - 91s 517ms/step - loss: 0.6965 - accuracy: 0.5095 - val_loss: 0.6960 - val_accuracy: 0.5071\n",
            "Epoch 7/10\n",
            "177/177 [==============================] - 97s 544ms/step - loss: 0.6957 - accuracy: 0.4996 - val_loss: 0.7154 - val_accuracy: 0.4929\n",
            "Epoch 8/10\n",
            "177/177 [==============================] - 101s 572ms/step - loss: 0.6967 - accuracy: 0.4912 - val_loss: 0.6968 - val_accuracy: 0.4929\n",
            "Epoch 9/10\n",
            "177/177 [==============================] - 93s 524ms/step - loss: 0.6955 - accuracy: 0.4968 - val_loss: 0.6940 - val_accuracy: 0.5071\n",
            "Epoch 10/10\n",
            "177/177 [==============================] - 97s 549ms/step - loss: 0.6959 - accuracy: 0.4951 - val_loss: 0.6960 - val_accuracy: 0.5071\n",
            "45/45 [==============================] - 6s 135ms/step - loss: 0.6960 - accuracy: 0.5071\n",
            "Testing Accuracy is 50.70621371269226 \n",
            "45/45 [==============================] - 8s 159ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Against       0.51      1.00      0.67       718\n",
            "       Favor       0.00      0.00      0.00       698\n",
            "\n",
            "    accuracy                           0.51      1416\n",
            "   macro avg       0.25      0.50      0.34      1416\n",
            "weighted avg       0.26      0.51      0.34      1416\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for multiple targets\n",
        "for i in df[\"Target\"].unique():\n",
        "  tar = df[df[\"Target\"]==i]\n",
        "  print(\"*******************\",i,\"*********************\")\n",
        "  rnn_fasttext(tar,model)"
      ],
      "metadata": {
        "id": "46JXTCiirAXM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for total data set\n",
        "rnn_fasttext(df,model)"
      ],
      "metadata": {
        "id": "K-ZvdZnAXNnm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}